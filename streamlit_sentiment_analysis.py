import numpy as np
import pandas as pd
import pickle
import streamlit as st
from test_preprocessing import process_text, process_special_word, process_postag_thesea, remove_stopwords
from wordcloud import WordCloud


# Load a dictionary file as key-value pairs
def load_dictionary(file_path):
    """Load a dictionary from a file with tab-separated key-value pairs."""
    with open(file_path, 'r', encoding="utf8") as file:
        lines = file.read().split('\n')
    dictionary = {}
    for line in lines:
        if '\t' in line:
            key, value = line.split('\t')
            dictionary[key] = str(value)
    return dictionary


# Load a list from a file
def load_list(file_path):
    """Load a list from a file."""
    with open(file_path, 'r', encoding="utf8") as file:
        return file.read().split('\n')


# Load resources only once
emoji_dict = load_dictionary('./emojicon.txt')  
teen_dict = load_dictionary('./teencode.txt')  
english_dict = load_dictionary('./english-vnmese.txt')  
wrong_lst = load_list('./wrong-word.txt')  
stopword_lst = load_list('./vietnamese-stopwords.txt')  

# Load models once
with open('tfidf.pkl', 'rb') as file:
    tfidf = pickle.load(file)

with open('logistic_regression_model.pkl', 'rb') as file:
    logistics_regression = pickle.load(file)


# Streamlit UI Setup
st.title("Data Science Project")
st.markdown("# :rainbow[Sentiment Analysis]")

menu = ["Váº¥n Ä‘á» kinh doanh", "XÃ¢y dá»±ng model", "TÃ¬m kiáº¿m má»›i","PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ sáº£n pháº©m"]
choice = st.sidebar.selectbox('Menu', menu)
st.sidebar.write("""#### ThÃ nh viÃªn thá»±c hiá»‡n:
                 LÆ°Æ¡ng NhÃ£ HoÃ ng HÃ  & Pháº¡m BÃ­ch Nháº­t""")
st.sidebar.write("""#### Giáº£ng viÃªn hÆ°á»›ng dáº«n: 
                 CÃ´ Khuáº¥t ThÃ¹y PhÆ°Æ¡ng""")
st.sidebar.write("""#### Thá»i gian thá»±c hiá»‡n: 
                 14/12/2024""")

# Business problem page
if choice == 'Váº¥n Ä‘á» kinh doanh':    
    st.markdown("## Váº¥n Ä‘á» kinh doanh")
    multi = """
    ##### Nháº­n diá»‡n, phÃ¢n loáº¡i pháº£n há»“i cá»§a khÃ¡ch hÃ ng trÃªn website Hasaki.vn.
    ##### Tá»« nhá»¯ng Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng, giÃºp nhÃ£n hÃ ng hiá»ƒu khÃ¡ch hÃ ng Ä‘Ã¡nh giÃ¡ gÃ¬ vá» sáº£n pháº©m, tá»« Ä‘Ã³ Ä‘Æ°a ra cÃ¡c káº¿ hoáº¡ch cáº£i thiá»‡n cháº¥t lÆ°á»£ng sáº£n pháº©m cÅ©ng nhÆ° cÃ¡c dá»‹ch vá»¥ Ä‘i kÃ¨m.
    """
    st.markdown(multi)  
    st.write("""##### Thá»±c hiá»‡n: Sá»­ dá»¥ng Machine Learning (Random Forest).""")
    st.image("Sentiment-analysis.jpg")


# Model Building page
elif choice == 'XÃ¢y dá»±ng model':
    data = pd.read_csv("Danh_gia.csv", encoding='utf-8')
    st.subheader("XÃ¢y dá»±ng model")
    st.write("##### 1. Thu tháº­p vÃ  Ä‘á»c data")
    st.dataframe(data.head(10))
    st.image('PhanPhoiRating.png')

    st.write("##### 2. Data preprocessing")
    multi2 = '''##### CÃ¡c bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u bao gá»“m:
    * Chuyá»ƒn nháº­n xÃ©t tá»« viáº¿t hoa thÃ nh viáº¿t thÆ°á»ng
    * Loáº¡i bá» cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t trong cÃ¢u (dáº¥u, sá»‘, stop-word, khoáº£ng tráº¯ng)
    * Thay emoji vÃ  teen-code báº±ng tá»« tiáº¿ng viá»‡t chuáº©n
    * Ná»‘i cÃ¡c cÃ¢u láº¡i vá»›i nhau, vÃ  ngÄƒn cÃ¡ch giá»¯a cÃ¡c cÃ¢u báº±ng dáº¥u cháº¥m
    PhÃ¢n loáº¡i nháº­n xÃ©t thÃ nh positive (tá»« 4 â­ â¬†ï¸) vÃ  negative (tá»« 3 â­â¬‡ï¸)
    Over-sampling cÃ¡c nháº­n xÃ©t negative Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u (hÆ¡n 70% nháº­n xÃ©t lÃ  positive)
    '''
    st.markdown(multi2)
    st.write("##### Output dá»¯ liá»‡u sau khi tiá»n xá»­ lÃ½")
    Output = pd.read_csv("comment_df.csv", encoding='utf-8')
    st.dataframe(Output.head(10))

    st.write("##### 3. Build model")
    st.write("##### 3.1. Modeling with traditional machine learning")
    multi3 = '''
    models = { \n
    "Naive Bayes": MultinomialNB(),\n
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42), \n
    "Random Forest": RandomForestClassifier(random_state=42), \n
    }
    '''
    st.markdown(multi3)
    st.image('ROC_curve.png')


# New search page
elif choice == 'TÃ¬m kiáº¿m má»›i':
    st.subheader("Chá»n input")
    flag = False
    lines = None
    type = st.radio("Upload data or Input data?", options=("Upload", "Input"))
    # File upload
        
    if type=="Upload":
        uploaded_file = st.file_uploader("Choose a file", type=['txt', 'csv'])
        if uploaded_file is not None:
            # Read and decode the file content using UTF-8
            content = uploaded_file.read().decode('utf-8')
            
            # Process the content based on file type
            if uploaded_file.name.endswith('.txt'):
                # Split the content into lines (create a list from lines)
                lines = content.splitlines()  # This splits by newlines
                st.write("Loaded lines from text file:")
                st.write(lines)
            
            elif uploaded_file.name.endswith('.csv'):
                # Load CSV file into a DataFrame and convert to list
                # Assuming the CSV has one column, or you're interested in the first column
                lines = pd.read_csv(io.StringIO(content), header=None)[0].tolist()  # Convert first column to list
                st.write("Loaded data from CSV file:")
                st.write(lines)
            flag = True

    # Direct input
    if type == "Input":        
        content = st.text_area(label="Input your content:")
        if content:
            lines = content.split('\n')
            flag = True

    # Process and predict
    if flag:
        preprocessed_lines = [
            remove_stopwords(
                process_postag_thesea(
                    process_special_word(process_text(line, emoji_dict, teen_dict, english_dict, wrong_lst))
                ),
                stopword_lst
            )
            for line in lines
        ]
        # After loading, you can process the list (e.g., preprocessing)
        st.write("Danh sÃ¡ch comment:")
        st.write(lines)
        st.write("Ná»™i dung Ä‘Ã£ xá»­ lÃ½:")
        st.code("\n".join(preprocessed_lines))  

        # Predict sentiment
        x_new = tfidf.transform(preprocessed_lines)
        y_pred_new = logistics_regression.predict(x_new)
        sentiments = ["Positive" if pred == 1 else "Negative" for pred in y_pred_new]

        # Display prediction
        result_df = pd.DataFrame({
            'Original Text': lines,
            'Processed Text': preprocessed_lines,
            'Sentiment': sentiments
        })
        st.write("### Dá»± Ä‘oÃ¡n:")
        st.dataframe(result_df)
elif choice == 'PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ sáº£n pháº©m':
    st.markdown("## PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ sáº£n pháº©m")
    # Positive and negative words and emojis
    positive_words = [
        "thÃ­ch", "tá»‘t", "xuáº¥t sáº¯c", "tuyá»‡t vá»i", "á»•n", "tuyá»‡t", "ok", "okay",
        "hÃ i lÃ²ng", "Æ°ng Ã½", "hoÃ n háº£o", "cháº¥t lÆ°á»£ng", "nhanh", "tiá»‡n lá»£i", "dá»… sá»­ dá»¥ng", 
        "hiá»‡u quáº£", "áº¥n tÆ°á»£ng", "ná»•i báº­t", "thÃ¢n thiá»‡n", "cao cáº¥p", "Ä‘á»™c Ä‘Ã¡o", "ráº¥t tá»‘t", 
        "ráº¥t thÃ­ch", "táº­n tÃ¢m", "Ä‘Ã¡ng tin cáº­y", "Ä‘áº³ng cáº¥p", "háº¥p dáº«n", "an tÃ¢m", "thÃºc Ä‘áº©y", 
        "cáº£m Ä‘á»™ng", "phá»¥c vá»¥ tá»‘t", "lÃ m hÃ i lÃ²ng", "gÃ¢y áº¥n tÆ°á»£ng", "ná»•i trá»™i", "sÃ¡ng táº¡o", 
        "phÃ¹ há»£p", "táº­n tÃ¢m", "hiáº¿m cÃ³", "cáº£i thiá»‡n", "hoÃ  nhÃ£", "chÄƒm chá»‰", "cáº©n tháº­n", 
        "vui váº»", "sÃ¡ng sá»§a", "hÃ o há»©ng", "Ä‘am mÃª", "vá»«a váº·n", "Ä‘Ã¡ng tiá»n"
    ]
    negative_words = [
        "kÃ©m", "tá»‡", "buá»“n", "chÃ¡n", "khÃ´ng dá»… chá»‹u", "khÃ´ng cháº¥t lÆ°á»£ng", "kÃ©m cháº¥t lÆ°á»£ng", 
        "khÃ´ng thÃ­ch", "khÃ´ng á»•n", "khÃ´ng há»£p", "khÃ´ng Ä‘Ã¡ng tin cáº­y", "khÃ´ng chuyÃªn nghiá»‡p",
        "khÃ´ng pháº£n há»“i", "khÃ´ng an toÃ n", "khÃ´ng phÃ¹ há»£p", "khÃ´ng thÃ¢n thiá»‡n", "khÃ´ng linh hoáº¡t", 
        "khÃ´ng Ä‘Ã¡ng giÃ¡", "khÃ´ng áº¥n tÆ°á»£ng", "khÃ´ng tá»‘t", "cháº­m", "khÃ³ khÄƒn", "phá»©c táº¡p", "khÃ³ chá»‹u", 
        "gÃ¢y khÃ³ dá»…", "rÆ°á»m rÃ ", "tháº¥t báº¡i", "tá»“i tá»‡", "khÃ³ xá»­", "khÃ´ng thá»ƒ cháº¥p nháº­n", "tá»“i tá»‡",
        "khÃ´ng rÃµ rÃ ng", "khÃ´ng cháº¯c cháº¯n", "rá»‘i ráº¯m", "khÃ´ng tiá»‡n lá»£i", "khÃ´ng Ä‘Ã¡ng tiá»n", 'khÃ´ng hÃ i lÃ²ng', 
        'khÃ´ng Ä‘Ã¡ng', 'quÃ¡ tá»‡', 'ráº¥t tá»‡', 'tháº¥t vá»ng', 'chÃ¡n', 'tá»‡ háº¡i', 'kinh khá»§ng', 'khÃ´ng Æ°ng Ã½'
    ]
    # Emoji lists
    positive_emojis = ["ðŸ˜„", "ðŸ˜ƒ", "ðŸ˜€", "ðŸ˜", "ðŸ˜†", "ðŸ˜…", "ðŸ¤£", "ðŸ˜‚", "ðŸ™‚", "ðŸ™ƒ", "ðŸ˜‰", "ðŸ˜Š", "ðŸ˜‡", "ðŸ¥°", "ðŸ˜"]
    negative_emojis = ["ðŸ˜ž", "ðŸ˜”", "ðŸ™", "â˜¹ï¸", "ðŸ˜•", "ðŸ˜¢", "ðŸ˜­", "ðŸ˜–", "ðŸ˜£", "ðŸ˜©", "ðŸ˜ ", "ðŸ˜¡", "ðŸ¤¬", "ðŸ˜¤", "ðŸ˜°", "ðŸ˜¨"]

    def find_words(document, list_of_words):
        document_lower = document.lower()
        word_count = 0
        word_list = []
        for word in list_of_words:
            if word in document_lower:
                word_count += document_lower.count(word)
                word_list.append(word)
        return word_count, word_list

    # Display list of products
    df_products = pd.read_csv('San_pham.csv')
    st.session_state.df_products = df_products

    if 'selected_ma_san_pham' not in st.session_state:
        st.session_state.selected_ma_san_pham = None

    product_options = [(row['ten_san_pham'], row['ma_san_pham']) for index, row in df_products.iterrows()]
    selected_product = st.selectbox(
        "Chá»n sáº£n pháº©m",
        options=product_options,
        format_func=lambda x: x[0]  # Hiá»ƒn thá»‹ tÃªn sáº£n pháº©m
    )

    st.session_state.selected_ma_san_pham = selected_product[1]
    if st.session_state.selected_ma_san_pham:
        st.write("ma_san_pham:", st.session_state.selected_ma_san_pham)
        selected_product_info = df_products[df_products['ma_san_pham'] == st.session_state.selected_ma_san_pham]
        if not selected_product_info.empty:
            st.write('### Báº¡n vá»«a chá»n:')
            st.write('### ', selected_product_info['ten_san_pham'].values[0])
            product_description = selected_product_info['mo_ta'].values[0]
            truncated_description = ' '.join(product_description.split()[:100])
            st.write('#### ThÃ´ng tin:')
            st.write(truncated_description, '...')

        # Load the 'comment_df.csv' file containing the ratings and comments
        comment_data = pd.read_csv('comment_df.csv', encoding='utf-8')
        selected_product_comments = comment_data[comment_data['ma_san_pham'] == st.session_state.selected_ma_san_pham]

        # Calculate the average rating for the selected product
        average_rating = selected_product_comments['so_sao'].mean()
        st.write(f"### Äiá»ƒm trung bÃ¬nh cá»§a sáº£n pháº©m {selected_product_info['ten_san_pham'].values[0]}: {average_rating:.2f} ")

        # Split comments into positive and negative based on ratings
        positive_comments = selected_product_comments[selected_product_comments['so_sao'] >= 4]['noi_dung_binh_luan']
        negative_comments = selected_product_comments[selected_product_comments['so_sao'] <= 3]['noi_dung_binh_luan']

        positive_comments_text = ' '.join(positive_comments)
        negative_comments_text = ' '.join(negative_comments)

        # Optionally: Display the comments themselves
        st.subheader("Positive Comments")
        st.write(positive_comments)

        st.subheader("Negative Comments")
        st.write(negative_comments)

        # Count words and emojis for positive and negative words
        positive_word_count, positive_word_list = find_words(' '.join(positive_comments), positive_words)
        st.write("Sá»‘ lÆ°á»£ng tá»« tÃ­ch cá»±c:", positive_word_count)
        st.write("Danh sÃ¡ch tá»« tÃ­ch cá»±c:", positive_word_list)

        negative_word_count, negative_word_list = find_words(' '.join(negative_comments), negative_words)
        st.write("Sá»‘ lÆ°á»£ng tá»« tiÃªu cá»±c:", negative_word_count)
        st.write("Danh sÃ¡ch tá»« tiÃªu cá»±c:", negative_word_list)

        positive_emoji_count, positive_emoji_list = find_words(' '.join(positive_comments), positive_emojis)
        st.write("Sá»‘ lÆ°á»£ng emoji tÃ­ch cá»±c:", positive_emoji_count)
        st.write("Danh sÃ¡ch emoji tÃ­ch cá»±c:", positive_emoji_list)

        negative_emoji_count, negative_emoji_list = find_words(' '.join(negative_comments), negative_emojis)
        st.write("Sá»‘ lÆ°á»£ng emoji tiÃªu cá»±c:", negative_emoji_count)
        st.write("Danh sÃ¡ch emoji tiÃªu cá»±c:", negative_emoji_list)
        
    # Generate word clouds for positive and negative comments
    if not positive_comments.empty:
        # Join all positive comments into a single string before generating the word cloud
        positive_comments_text = ' '.join(positive_comments)
        positive_wordcloud = WordCloud(width=800, height=400, background_color='white',max_words=30).generate(positive_comments_text)
        st.subheader("Word Cloud for Positive Comments")
        st.image(positive_wordcloud.to_array())

    if not negative_comments.empty:
        # Join all negative comments into a single string before generating the word cloud
        negative_comments_text = ' '.join(negative_comments)
        negative_wordcloud = WordCloud(width=800, height=400, background_color='white',max_words=30).generate(negative_comments_text)
        st.subheader("Word Cloud for Negative Comments")
        st.image(negative_wordcloud.to_array())
